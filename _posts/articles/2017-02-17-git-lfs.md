---
layout: post
title: "Accessing sequence data for reproducible science"
author: Deren
modified:
categories: articles
excerpt: "I always include SRA accession IDs for archived sequence data
when I publish a paper, but I rarely use the sra-tools software to access
the data. Here I learn a bit more about it. "
comments: true
image:
  feature: header_ped.png
share: true
tags: [reproducibility, github]
date: 2017-02-17
---

#### Why is it so hard?
Access to data remains a major hurdle to reproducibility in science today
despite the availability of large-scale repositories for data storage, and
even journal guidelines requiring data archiving. It is difficult to find
statistics of just *how often data are not archived* for published studies,
but guessing from my own experience in attempting to reproduce other published
studies, it appears very commonplace.
For example, when assembling a list of studies for a recent meta-analysis I
found that only about half had raw sequence data available in a sequence read
archive (e.g., SRA, ERA). Data were sometimes available elsewhere,
in non-indexed locations like DRYAD or personal websites,
while many others were not available at all.

There are many reasons for not submitting data to SRA, and if I had to guess,
for most people it is not for fear of being scooped, or to avoid having their
work checked, but rather, because uploading data to NCBI is kind of a
*massively time-consuming and soul-crushing exercise.*
It requires entering pages upon pages of metadata by hand into arcane web forms
to define various objects that are continually referenced by redundant
names or prefix tags (e.g., SUB, SAMN, SRX, SRP, PRJNA, BioSamples and BioProjects),
and which have a relational structure that defies understanding
(e.g., 1 SRA can have 4 SRXs which produce data for 96 SRRs from
96 SRSs for 4 SRPs; See table below).
And make sure you don't screw anything up along the way, because if you
enter a wrong term into a form, even before hitting submit, many
terms cannot be edited, and the submission cannot be deleted -- your only
recourse seems to be to contact an administrator about correcting it for you.

| Accession Prefix	| Accession Name	|   Definition	   |   Example   |
|:------------------| :-------------- | :--------------- |:------------|
| SRX  | *Experiment* | Metadata about library, platform, selection.    | link |
| SRR  | *Run*        | **The actual sequence data for an experiment**. | link |
| SRP  | *Study*      | Metadata about project (BioProject).            | link |
| SRS  | *Sample*     | Metadata about the physical Sample (BioSample)  | link |
| SRZ  | *Analysis*   | Mapped/aligned reads file (BAM) & metadata.     | link |
| SRA  | *Submission* | Metadata of other 5 linked objects.             | link |


#### Accessing the raw data
So what are the benefits of having your data in the SRA? Well, believe it or not
the metadata is useful. But furthermore, having all sequence data archived in a
uniform way, in a searchable database, is useful because it allows for
uniform methods to be developed to access the data. For the purpose of
producing reproducible documentation, then, like a jupyter-notebook that
lays out all steps in a bioinformatic analysis,
a single code block could be written at the beginning of a document to query
and download all of the sequence data for the project.

This would be a huge advance over what is commonly available today
for doing reproducible science, which is typically just a verbal instruction
to "go get the raw data" before you start. In fact, I would guess that even
most of the studies with the best reproducible documentation tend to start
from the point of assuming that the user has properly accessed the data,
renamed files appropriately, and grouped them into a correct directory
structure, etc. Instead, the reproducible code should download the data and
format it for the user so that it is ready for analysis.

#### Tools for accessing data
I've tried to hack my way through the process of accessing data from SRA
in several recent projects using my own scripts simply because I did not
like the proprietary software offered by SRA
([sra-tools](http://ncbi.github.io/sra-tools/)). Until about six months ago
it really wasn't very easy to install, or even find the software, and so it
seemed easier to me to just use tools that are available to everyone,
like `curl`. Fortunately, however, the state of software
management has changed rapidly in recent years, and now sra-tools is on github,
and you even install it easily with a simple `conda install` command.
So I guess I'm back on board with sra-tools now, and so I'm making this post
while I try to figure out how this thing actually works.

#### Getting raw fastq data with sra-tools
There are two main software packages for accessing data from NCBI/Entrez which
are now easily available through conda.
```bash
>>> conda install -c bioconda sra-tools
>>> conda install -c bioconda entrez-direct
```

The key program provided in `sra-tools` is called `fastq-dump`, which is used
to download data from SRA, and the main tool in `entrez-tools` is called
`esearch`, which queries metadata from NCBI. If you wanted to get the data from
a single Sample Accession (SRR) you can download its fastq data easily with
`fastq-dump.`:

```bash
>>> fastq-dump SRR1754715
```

If you want to download data for an entire project, however, you'll need to
query the project metadata using `esearch`. With this you can download all of the
Run IDs (SRR) under a project Accession ID (SRP; which is the ID typically
reported in a publication). It just requires a bit of tricky bash scripting.
If you google around you can find some nice recipes for this, which is what I did to find the
example which I'll explain below. In this case we use the `esearch` command
to search SRA for all of the information in the project accession we are interested
in, and then pipe the results into the program `efetch` which is used to parse
out the runinfo:

```bash
>>> esearch -db sra -query SRP021469 | efetch --format runinfo | cut -d ',' -f 1
```

The following is printed to stdout:
```yaml
 1
Run
SRR1754715
SRR1754720
SRR1754721
SRR1754722
SRR1754723
SRR1754724
SRR1754725
SRR1754726
SRR1754727
SRR1754728
SRR1754729
SRR1754730
SRR1754731
```

Breaking down the command into smaller bits, you can see what each call is doing
before piping to the next:

```bash
>>> esearch -db sra -query SRP021469 |      \  ## search SRA for SRP021469
      efetch --format runinfo |             \  ## parse the runinfo
      cut -d ',' -f 1 |                     \  ## split on ',' & grab 1st thing
      grep SRR |                            \  ## grab lines starting with 'SRR'
      xargs fastq-dump --gzip               \  ## run fastq-dump on all SRR #s
                       --outdir fastqs/     \  ## gzip compress & put into dir
```

#### Renaming files
SRA renames the files so that they are named according to their SRR accession
IDs. However, we will often want the files to be named according the ID that the
researchers originally provided for the files, which will likely be composed
of a taxonomic name and collection number. To rename the files based
on their original accession IDs we'll need to grab the 30th element from runinfo.
The command below shows what this looks like for the example SRP accession.

```bash
>>> esearch -db sra -query SRP021469 | efetch --format runinfo | cut -d ',' -f 30
```

The following is printed to stdout:
```yaml
 30
SampleName
29154_superba
30556_thamno
41954_cyathophylloides
41478_cyathophylloides
39618_rex
40578_rex
38362_rex
35855_rex
33588_przewalskii
33413_thamno
32082_przewalskii
30686_cyathophylla
35236_rex
```

OK, so now we want to download the sequence data *and* rename the files
according to the ``SampleName`` info that we parse from the metadata.
To do this I broke the command into two separate calls here to
simplify it a bit. We will pass the `--accession` flag to `fastq-dump` which
tells it we want to rename the file and as an argument to it
we provide the `SampleName`.

```bash
## store elements 1 & 30 into a string/list
>>> vars=$(esearch -db sra -query SRP021469 |  
           efetch --format runinfo |           
           cut -d ',' -f 1,30 |                  
           grep SRR)         

## pass each pair of elements to fastq-dump
>>> for var in $vars
    do
        SRR=$(echo $var | cut -d ',' -f 1);
        ACC=$(echo $var | cut -d ',' -f 2);
        fastq-dump $SRR --accession $ACC --outdir fastqs/ --gzip;
    done
```

The resulting data files:
```bash
>>> ls -l fastqs/
```

```yaml
-rw-rw-r-- 1 deren deren  42M Feb 17 20:00 29154_superba.fastq.gz
-rw-rw-r-- 1 deren deren  86M Feb 17 20:00 30556_thamno.fastq.gz
-rw-rw-r-- 1 deren deren 136M Feb 17 20:01 41954_cyathophylloides.fastq.gz
-rw-rw-r-- 1 deren deren 128M Feb 17 20:02 41478_cyathophylloides.fastq.gz
-rw-rw-r-- 1 deren deren  50M Feb 17 20:03 39618_rex.fastq.gz
-rw-rw-r-- 1 deren deren 100M Feb 17 20:04 40578_rex.fastq.gz
-rw-rw-r-- 1 deren deren  82M Feb 17 20:05 38362_rex.fastq.gz
-rw-rw-r-- 1 deren deren  84M Feb 17 20:06 35855_rex.fastq.gz
-rw-rw-r-- 1 deren deren  61M Feb 17 20:06 33588_przewalskii.fastq.gz
-rw-rw-r-- 1 deren deren  40M Feb 17 20:06 33413_thamno.fastq.gz
-rw-rw-r-- 1 deren deren  58M Feb 17 20:07 32082_przewalskii.fastq.gz
-rw-rw-r-- 1 deren deren  80M Feb 17 20:08 30686_cyathophylla.fastq.gz
-rw-rw-r-- 1 deren deren 108M Feb 17 20:09 35236_rex.fastq.gz
```

Success!


#### Is this the best way to do things?
For reasonably small sized data sets it does seem like overkill. For example,
the above data set is relatively small and so we use it as the example
empirical data set in the [ipyrad documentation](http://ipyrad.rtfd.io). There
we provide just a simple curl command to download these same files which are
already named appropriately from a publicly available link:

```bash
## curl grabs the data from a public dropbox url
>>> curl -LkO https://dl.dropboxusercontent.com/u/2538935/example_empirical_rad.tar.gz

## the tar command decompresses the data directory
>>> tar -xvzf example_empirical_rad.tar.gz
```

The obvious answer is that the `sra-tools` workflow is better, since the data
are "permanently" archived, but the code certainly does seem a bit
more complicated, which can scare users away from using it. For example,
for the ipyrad documentation I'll probably stick with the Dropbox link for now,
but if I used the data for a published paper I would use the sra-tools
setup. The more people get used to using the sra-tools, however,
and become familiar with it, I suppose it'll become less scary and I'd feel
more comfortable using it more broadly.
