---
layout: post
title: "Accessing sequence data for reproducible science"
author: Deren
modified:
categories: articles
excerpt: "SRA accession IDs are typically provided in published papers
but using the sra-tools software can be a pain. Here I document a few tips
to remind myself how to use it. "
comments: true
image:
  feature: header_ped.png
share: true
tags: [reproducibility, github]
date: 2017-02-17
---

#### What's the holdup?
Access to data remains a major hurdle to reproducibility in science today
despite the increased availability of large-scale repositories for data storage, 
and even journal policies that require data archiving. It is difficult to find
statistics of just *how often data are not archived* in published studies,
but guessing from my own experience in attempting to reproduce other published
studies, it seems quite commonplace.
<!-- For example, when assembling a list of studies for a recent meta-analysis I
found that only about half had raw sequence data available in a sequence read
archive (e.g., SRA, ERA). Data were sometimes available elsewhere,
in non-indexed locations like DRYAD or personal websites,
while many others were not available at all. -->

There are many reasons for not submitting data to SRA, and if I had to guess,
the reason we often do not submit data is not for fear of being 
scooped, or to prevent others from reusing our data, but rather, 
because uploading data to NCBI is a *fairly time-consuming and soul-crushing exercise.*
It requires entering pages upon pages of metadata by hand into arcane web forms
or spreadsheets to define various objects that are continually referenced by 
redundant names or prefix tags (e.g., SUB, SAMN, SRX, SRP, PRJNA, BioSamples and BioProjects), and which have a relational structure that defies understanding
(e.g., 1 SRA can have 4 SRXs which produce data for 96 SRRs from
96 SRSs for 4 SRPs; See table below).

| Accession Prefix	| Accession Name	|   Definition	   |   Example   |
|:------------------| :-------------- | :--------------- |:------------|
| SRX  | *Experiment* | Metadata about library, platform, selection.    | link |
| SRR  | *Run*        | **The actual sequence data for an experiment**. | link |
| SRP  | *Study*      | Metadata about project (BioProject).            | link |
| SRS  | *Sample*     | Metadata about the physical Sample (BioSample)  | link |
| SRZ  | *Analysis*   | Mapped/aligned reads file (BAM) & metadata.     | link |
| SRA  | *Submission* | Metadata of other 5 linked objects.             | link |


#### Accessing the raw data
The distinguishing benefit of having all sequence data archived in SRA, along with
its metadata, is that we can easily develop a set of uniform scripts to access
data regardles of it's format, or distribution among samples and lanes of sequencing.
For the purpose of creating reproducible documentation, then, 
such as a jupyter-notebook that includes all steps in a bioinformatic analysis,
a single code block could be written at the beginning of a document to query
and download all of the sequence data for the project by simply providing the 
project ID. 

This would be a huge advance over what is commonly available today
for doing reproducible science, which is typically just a verbal instruction
to "go get the raw data" before you start. Even great examples of reproducible 
science often start from the point of assuming that the user has properly 
accessed the data, renamed files appropriately, and grouped them into a 
correct directory structure, etc. 
Instead, reproducible code should itself include the necessary code 
to download the data and format so it is ready for downstream analysis.

#### Tools for accessing data
I've tried to hack my way through the process of accessing data from SRA
in several recent projects using my own scripts in an effort to avoid having
to use the proprietary software offered by SRA
([sra-tools](http://ncbi.github.io/sra-tools/)), which until a few months ago
was fairly difficult to install, or even find online, and so it
seemed easier to simply write `curl` scripts to access the fastq files. 
Fortunately, however, the state of software management has changed rapidly in recent years, and now sra-tools is on github, and you can even install it easily 
with a simple `conda install` command. So I decided to give it another look, 
which is what this post is about. 

#### Getting raw fastq data with sra-tools
There are two main software packages for accessing data from NCBI/Entrez which
are now easily available through conda.
```bash
>>> conda install -c bioconda sra-tools
>>> conda install -c bioconda entrez-direct
```

The key program in `sra-tools` is a tool called `fastq-dump`, which is used
to download data from SRA, and the main tool in `entrez-tools` is called
`esearch`, which queries metadata from NCBI. If you wanted to get the data from
a single Sample Accession (SRR) you can download its fastq data easily with
`fastq-dump.`:

```bash
>>> fastq-dump SRR1754715
```

If you want to download data for an entire project, however, you'll need to
query the project metadata using `esearch`. With this you can download all of the
Run IDs (SRR) under a project Accession ID (SRP; which is the ID typically
reported in a publication). It just requires a bit of tricky bash scripting.
If you google around you can find some nice recipes for this, which is what I did to find the example which I'll explain below. In this case we use the `esearch` command
to search SRA for all of the information in the project accession we are interested
in, and then pipe the results into the program `efetch` which is used to parse
out the runinfo:

```bash
>>> esearch -db sra -query SRP021469 | efetch --format runinfo | cut -d ',' -f 1
```

The following is printed to stdout:
```yaml
 1
Run
SRR1754715
SRR1754720
SRR1754721
SRR1754722
SRR1754723
SRR1754724
SRR1754725
SRR1754726
SRR1754727
SRR1754728
SRR1754729
SRR1754730
SRR1754731
```

Breaking down the command into smaller bits, you can see what each call is doing
before piping to the next:

```bash
>>> esearch -db sra -query SRP021469 |      \  ## search SRA for SRP021469
      efetch --format runinfo |             \  ## parse the runinfo
      cut -d ',' -f 1 |                     \  ## split on ',' & grab 1st thing
      grep SRR |                            \  ## grab lines starting with 'SRR'
      xargs fastq-dump --gzip               \  ## run fastq-dump on all SRR #s
                       --outdir fastqs/     \  ## gzip compress & put into dir
```

#### Renaming files
SRA renames the files so that they are named according to their SRR accession
IDs. However, we will often want the files to be named according the ID that the
researchers originally provided for the files, which will likely be composed
of a taxonomic name and collection number. To rename the files based
on their original accession IDs we'll need to grab the 30th element from runinfo.
The command below shows what this looks like for the example SRP accession.

```bash
>>> esearch -db sra -query SRP021469 | efetch --format runinfo | cut -d ',' -f 30
```

The following is printed to stdout:
```yaml
 30
SampleName
29154_superba
30556_thamno
41954_cyathophylloides
41478_cyathophylloides
39618_rex
40578_rex
38362_rex
35855_rex
33588_przewalskii
33413_thamno
32082_przewalskii
30686_cyathophylla
35236_rex
```

OK, so now we want to download the sequence data *and* rename the files
according to the ``SampleName`` info that we parse from the metadata.
To do this I broke the command into two separate calls here to
simplify it a bit. We will pass the `--accession` flag to `fastq-dump` which
tells it we want to rename the file and as an argument to it
we provide the `SampleName`.

```bash
## store elements 1 & 30 into a string/list
>>> vars=$(esearch -db sra -query SRP021469 |  
           efetch --format runinfo |           
           cut -d ',' -f 1,30 |                  
           grep SRR)         

## pass each pair of elements to fastq-dump
>>> for var in $vars
    do
        SRR=$(echo $var | cut -d ',' -f 1);
        ACC=$(echo $var | cut -d ',' -f 2);
        fastq-dump $SRR --accession $ACC --outdir fastqs/ --gzip;
    done
```

The resulting data files:
```bash
>>> ls -l fastqs/
```

```yaml
-rw-rw-r-- 1 deren deren  42M Feb 17 20:00 29154_superba.fastq.gz
-rw-rw-r-- 1 deren deren  86M Feb 17 20:00 30556_thamno.fastq.gz
-rw-rw-r-- 1 deren deren 136M Feb 17 20:01 41954_cyathophylloides.fastq.gz
-rw-rw-r-- 1 deren deren 128M Feb 17 20:02 41478_cyathophylloides.fastq.gz
-rw-rw-r-- 1 deren deren  50M Feb 17 20:03 39618_rex.fastq.gz
-rw-rw-r-- 1 deren deren 100M Feb 17 20:04 40578_rex.fastq.gz
-rw-rw-r-- 1 deren deren  82M Feb 17 20:05 38362_rex.fastq.gz
-rw-rw-r-- 1 deren deren  84M Feb 17 20:06 35855_rex.fastq.gz
-rw-rw-r-- 1 deren deren  61M Feb 17 20:06 33588_przewalskii.fastq.gz
-rw-rw-r-- 1 deren deren  40M Feb 17 20:06 33413_thamno.fastq.gz
-rw-rw-r-- 1 deren deren  58M Feb 17 20:07 32082_przewalskii.fastq.gz
-rw-rw-r-- 1 deren deren  80M Feb 17 20:08 30686_cyathophylla.fastq.gz
-rw-rw-r-- 1 deren deren 108M Feb 17 20:09 35236_rex.fastq.gz
```

Success!


#### Is this the best way to do things?
I think it depends on your purpose. For reasonably small sized tasks it does seem like overkill to have a large script just to download the data. For example,
the above data set is used as the example empirical data set in the 
[ipyrad documentation](http://ipyrad.rtfd.io). Because we want the tutorial to 
be as simple as possible, we don't want to have to ask users to install special software just to download the data, and then to have to run a complicated script. So in that case we simply ask users to copy and paste the simple curl command below to access the data. 

```bash
## curl grabs the data from a public dropbox url
>>> curl -LkO https://dl.dropboxusercontent.com/u/2538935/example_empirical_rad.tar.gz

## the tar command decompresses the data directory
>>> tar -xvzf example_empirical_rad.tar.gz
```

### Additional concerns of fastq-dump
I think the most annoying aspect of sra-tools is that it downloads intermediate files (in .sra format) and hides them in a somewhat cryptic location. Usually in `/home/user/ncbi/`, a folder which it creates if it does not already exist. And changing the default location of this is a bit complicated. So you'll want to remove the temporary files created in this directory after you are finished. This is especially problematic if you are working on a HPC cluster where you have limited disk space available in your home directory. In that case you will want to download the data directly to a scratch dir. 

### Pythonic implementations
The large Python module Biopython has tools to perform the same tasks, which can be nice if you are working in a Jupyter-notebook and the rest of your code is written in Python. For example, to download data the same as we did above using Python you could do the following: 


```bash
## conda install -c bioconda biopython
```

```python
## import package
import Bio

## query metainfo
handle = Bio.Entrez.esearch("SRP021469")
record = Bio.Entrez.read(handle)

## use a with statement? for handle

## set download location
## ...

## get files and rename
Bio.fastq_dump()
```

### The ipyrad way
While the biopython implementation can do a huge range of actions having to do with 
querying data from ncbi, it is fairly complicated and cumbersome if the only task you 
are interested in is to access nucleotide data, and specifically, RAD-seq data. And 
so we've implemented a much simpler method for querying and downloading data from sra
databases in ipyrad. 

```python
## import ipyrad analysis tools
import ipyrad.analysis as ipa

## initalize an sratools object
sra = ipa.sratools(accession="SRP021469", workdir="rawdata")

## view the meta-data accessed by the query
sra.fetch_runinfo()

## download the data, use force=True to overwrite existing files
sra.run()


```