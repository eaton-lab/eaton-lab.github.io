---
layout: post
title: "Accessing sequence data for reproducible science"
author: Deren
modified:
categories: articles
excerpt: "SRA accession IDs are typically provided in published papers
but using the sra-tools software can be a pain. Here I document a few tips
to remind myself how to use it. "
comments: true
image:
  feature: header_ped.png
share: true
tags: [reproducibility, github]
date: 2017-02-17
---


<h1 class="entry-subtitle">Diving into the SRA abyss</h1>
Access to data remains a major hurdle for reproducibility in science today
despite the increased availability of large-scale repositories for data storage, 
and even journal policies that require data archiving. This is particularly true
for genomic sequence data, where it can be really confusing to learn how to
upload large data files (e.g., requiring a checksum and FTP); and how to properly
tag with them appropriate meta-data. And so typically people just don't do it. 
Or if they do, the data is uploaded to an archive dump site, like DRYAD, where 
it is not easily searchable. 

There are many reasons for not submitting data to SRA, but surely the 
most common reason is because it's a completely *time-consuming and 
soul-crushing exercise.* It requires entering pages upon pages of meta-data 
by hand into arcane web forms or spread-sheets to define various 
objects that are continually referenced by redundant names or prefix tags
(e.g., SUB, SAMN, SRX, SRP, PRJNA, BioSamples and BioProjects), 
and which have a relational structure that defies understanding
(e.g., 1 SRA can have 4 SRXs which produce data for 96 SRRs from
96 SRSs for 4 SRPs; See table below; which I reference frequently when
trying to understand this stuff.)

| Prefix	| Accession Name	|   Definition	   |   Example   |
|:------------------| :-------------- | :--------------- |:------------|
| SRX  | *Experiment* | Metadata about library, platform, selection.    | link |
| SRR  | *Run*        | **The actual sequence data for an experiment**. | link |
| SRP  | *Study*      | Metadata about project (BioProject).            | link |
| SRS  | *Sample*     | Metadata about the physical Sample (BioSample)  | link |
| SRZ  | *Analysis*   | Mapped/aligned reads file (BAM) & metadata.     | link |
| SRA  | *Submission* | Metadata of other 5 linked objects.             | link |


<h1 class="entry-subtitle" 
  id="Accessing SRA data" 
  href="Accessing SRA data">
  Accessing Data
</h1>
The distinguishing benefit of having all sequence data archived in the SRA, 
with associated metadata, is that we can easily develop uniform scripts to access
the data regardless of its format or distribution among samples and lanes of sequencing.
For the purpose of creating reproducible documentation, then, 
a single code block can be written at the beginning of a document to query
and download all of the sequence data for a project. This would be a huge advance 
over what is commonly available today for doing reproducible science, which is 
typically just a verbal instruction to "go get the raw data" before you start. 
Even great examples of reproducible science often start from the point of assuming
that users have properly accessed the data, renamed files, and grouped them into a 
correct directory structure. Instead, reproducible code should itself include 
the necessary code to download the data from an online archive ready for 
downstream analysis.

<h1 class="entry-subtitle" 
  id="What about sra-tools" href="What about sra-tools">What about sra-tools?
</h1>

NCBI has developed a proprietary tool for accessing the SRA called 
([sra-tools](http://ncbi.github.io/sra-tools/)).
Historically, I avoided using it because it was atrociously difficult to access
and install and I didn't want to have to provide complex installation instructions 
at the beginning of my documents just so that others could install the software that 
I was using. *But*, now its 2017, and like any good software the sra-tools 
package is now available on conda, which makes it easy enough for anyone to use. 
To install the sra-tools along with another set of tools for searching genome 
databases (called entrez-tools), simply run the following command with conda.

```bash
>>> conda install -c bioconda sra-tools
>>> conda install -c bioconda entrez-direct
```

The key program in `sra-tools` is a tool called `fastq-dump`, which is used
to download data from SRA, and the main tool in `entrez-tools` is called
`esearch`, which queries meta-data from NCBI. If you wanted to get the data from
a single Sample Accession (SRR) you can download its fastq data easily with
`fastq-dump.`:

```bash
>>> fastq-dump SRR1754715
```

If you want to download data for an entire project, however, you need to
do some complex bash scripting. For example, below we query the "Study
accession" (SRP) to extract its metadata with `esearch` and from that 
we extract info using `efetch` to grab the "Run accessions (SRRs)", which we
could then pass into `fastq-dump` like above. 
If you google around you can find some nice recipes for this, which is what 
I did to find the example here. 

```bash
>>> esearch -db sra -query SRP021469 | efetch --format runinfo | cut -d ',' -f 1
```

The following "Run" accessions are printed to stdout:
```yaml
 1
Run
SRR1754715
SRR1754720
SRR1754721
SRR1754722
SRR1754723
SRR1754724
SRR1754725
SRR1754726
SRR1754727
SRR1754728
SRR1754729
SRR1754730
SRR1754731
```

The following command would download the SRRS as well. 
Breaking it down into smaller bits, you can see what each call is doing
before piping to the next:

```bash
>>> esearch -db sra -query SRP021469 |      \  ## search SRA for SRP021469
      efetch --format runinfo |             \  ## parse the runinfo
      cut -d ',' -f 1 |                     \  ## split on ',' & grab 1st thing
      grep SRR |                            \  ## grab lines starting with 'SRR'
      xargs fastq-dump --gzip               \  ## run fastq-dump on all SRR #s
                       --outdir fastqs/     \  ## gzip compress & put into dir
```

<h1 class="entry-subtitle" 
  id="What about sra-tools" href="What about sra-tools">What about sra-tools?
</h1>


#### Renaming files
SRA renames the files so that they are named according to their SRR accession
IDs. However, we will often want the files to be named according the ID that the
researchers originally provided for the files, which will likely be composed
of a taxonomic name and collection number. To rename the files based
on their original accession IDs we'll need to grab the 30th element from runinfo.
The command below shows what this looks like for the example SRP accession.

```bash
>>> esearch -db sra -query SRP021469 | efetch --format runinfo | cut -d ',' -f 30
```

The following is printed to stdout:
```yaml
 30
SampleName
29154_superba
30556_thamno
41954_cyathophylloides
41478_cyathophylloides
39618_rex
40578_rex
38362_rex
35855_rex
33588_przewalskii
33413_thamno
32082_przewalskii
30686_cyathophylla
35236_rex
```

OK, so now we want to download the sequence data *and* rename the files
according to the ``SampleName`` info that we parse from the metadata.
To do this I broke the command into two separate calls here to
simplify it a bit. We will pass the `--accession` flag to `fastq-dump` which
tells it we want to rename the file and as an argument to it
we provide the `SampleName`.

```bash
## store elements 1 & 30 into a string/list
>>> vars=$(esearch -db sra -query SRP021469 |  
           efetch --format runinfo |           
           cut -d ',' -f 1,30 |                  
           grep SRR)         

## pass each pair of elements to fastq-dump
>>> for var in $vars
    do
        SRR=$(echo $var | cut -d ',' -f 1);
        ACC=$(echo $var | cut -d ',' -f 2);
        fastq-dump $SRR --accession $ACC --outdir fastqs/ --gzip;
    done
```

The resulting data files:
```bash
>>> ls -l fastqs/
```

```yaml
-rw-rw-r-- 1 deren deren  42M Feb 17 20:00 29154_superba.fastq.gz
-rw-rw-r-- 1 deren deren  86M Feb 17 20:00 30556_thamno.fastq.gz
-rw-rw-r-- 1 deren deren 136M Feb 17 20:01 41954_cyathophylloides.fastq.gz
-rw-rw-r-- 1 deren deren 128M Feb 17 20:02 41478_cyathophylloides.fastq.gz
-rw-rw-r-- 1 deren deren  50M Feb 17 20:03 39618_rex.fastq.gz
-rw-rw-r-- 1 deren deren 100M Feb 17 20:04 40578_rex.fastq.gz
-rw-rw-r-- 1 deren deren  82M Feb 17 20:05 38362_rex.fastq.gz
-rw-rw-r-- 1 deren deren  84M Feb 17 20:06 35855_rex.fastq.gz
-rw-rw-r-- 1 deren deren  61M Feb 17 20:06 33588_przewalskii.fastq.gz
-rw-rw-r-- 1 deren deren  40M Feb 17 20:06 33413_thamno.fastq.gz
-rw-rw-r-- 1 deren deren  58M Feb 17 20:07 32082_przewalskii.fastq.gz
-rw-rw-r-- 1 deren deren  80M Feb 17 20:08 30686_cyathophylla.fastq.gz
-rw-rw-r-- 1 deren deren 108M Feb 17 20:09 35236_rex.fastq.gz
```

Success!


#### Is this the best way to do things?
I think it depends on your purpose. For reasonably small sized tasks it does seem like overkill to have a large script just to download the data. For example,
the above data set is used as the example empirical data set in the 
[ipyrad documentation](http://ipyrad.rtfd.io). Because we want the tutorial to 
be as simple as possible, we don't want to have to ask users to install special software just to download the data, and then to have to run a complicated script. So in that case we simply ask users to copy and paste the simple curl command below to access the data. 

```bash
## curl grabs the data from a public dropbox url
>>> curl -LkO https://dl.dropboxusercontent.com/u/2538935/example_empirical_rad.tar.gz

## the tar command decompresses the data directory
>>> tar -xvzf example_empirical_rad.tar.gz
```

### Additional concerns of fastq-dump
I think the most annoying aspect of sra-tools is that it downloads intermediate files (in .sra format) and hides them in a somewhat cryptic location. Usually in `/home/user/ncbi/`, a folder which it creates if it does not already exist. And changing the default location of this is a bit complicated. So you'll want to remove the temporary files created in this directory after you are finished. This is especially problematic if you are working on a HPC cluster where you have limited disk space available in your home directory. In that case you will want to download the data directly to a scratch dir. 

### Pythonic implementations
The large Python module Biopython has tools to perform the same tasks, which can be nice if you are working in a Jupyter-notebook and the rest of your code is written in Python. For example, to download data the same as we did above using Python you could do the following: 


```bash
## conda install -c bioconda biopython
```

```python
## import package
import Bio

## query metainfo
handle = Bio.Entrez.esearch("SRP021469")
record = Bio.Entrez.read(handle)

## use a with statement? for handle

## set download location
## ...

## get files and rename
Bio.fastq_dump()
```

### The ipyrad way
While the biopython implementation can do a huge range of actions having to do with 
querying data from ncbi, it is fairly complicated and cumbersome if the only task you 
are interested in is to access nucleotide data, and specifically, RAD-seq data. And 
so we've implemented a much simpler method for querying and downloading data from sra
databases in ipyrad. 

```python
## import ipyrad analysis tools
import ipyrad.analysis as ipa

## initalize an sratools object
sra = ipa.sratools(accession="SRP021469", workdir="rawdata")

## view the meta-data accessed by the query
sra.fetch_runinfo()

## download the data, use force=True to overwrite existing files
sra.run()


```